{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloom Filter\n",
    "\n",
    "A Bloom filter is a space-efficient probabilistic data structure, that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not â€“ in other words, a query returns either \"possibly in set\" or \"definitely not in set\". Elements can be added to the set, but not removed (though this can be addressed with the counting Bloom filter variant); the more items added, the larger the probability of false positives.\n",
    "\n",
    "Bloom proposed the technique for applications where the amount of source data would require an impractically large amount of memory if \"conventional\" error-free hashing techniques were applied. He gave the example of a hyphenation algorithm for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive disk accesses to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most unnecessary accesses. For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the disk accesses.\n",
    "<a href='https://arxiv.org/pdf/1803.04189.pdf'>Read More...</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing task!\n",
    "The bloom filter is an array initialized with all False of lenght hash_size, a huge prime number.\n",
    "To be sure that the probability of false positive is small enough we take $p\\approx\\big(1-e^{\\frac{-kn}{m}}\\big)^k \\approx 10^{-15}$.\n",
    "To do this, knowing that the numbers of passwords in passwords1.txt is $n=10^8$, we pick the follow values for hash_size $m=10000000019$ a huge prime number base on the chapter 1 of [book](https://books.google.it/books?id=ONU4tfT_GxcC&dq=algorithm+Umesh+Vazirani&hl=en&sa=X&ved=0ahUKEwiC1KPghormAhXIG5oKHZ_gBqkQ6AEIKTAA) titled Algorithm and hash_funcs_num $k=20$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert each password to an array corresponding ordinals of the length of the password.\n",
    "\n",
    "$$\\forall\\: \\text{hash function} \\: h_j \\:\\: \\text{with} \\: j \\in \\{0, \\dots ,k-1\\} \\:\\:\\:\\: h_j = \\sum_{i=0}^{19} ord_i C_{i,j} \\: mod \\: m $$\n",
    "\n",
    "Each $C_{i,j}$ is taken randomly from a uniform discret distribution ranging from $0$ to $m-1$\n",
    "\n",
    "To have a faster numerical computation we use numba package that can convert python function to be compiled on C/C++ that has the ability to run the function on GPU and CPU(Parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# numba is the package that can convert python function to be compiled on C/C++\n",
    "# that has the ability to run the function in on GPU and CPU(Parallel) that help\n",
    "# numerical computation to be done faster. REALLY FASTERRRRRRR \n",
    "from numba import jit\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "# PySpark is the package to run Map/Reduce procedure over Spark in python\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class keep the information about bloom filter like number of hash functions (K), bloom filter size (m)\n",
    "# length of the passwords to make the coefficient in hash functions, and store the bloom filter itself.  \n",
    "class hash_configuration():\n",
    "    def __init__(self, name, hash_size, string_size = 20, hash_funcs_num = 10):\n",
    "        self.name = name\n",
    "        self.string_size = string_size\n",
    "        self.hash_size = hash_size\n",
    "        self.hash_funcs_num = hash_funcs_num\n",
    "        self.coef = np.random.randint(0,self.hash_size-1,(self.hash_funcs_num, self.string_size), dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Python is Faster while communicates with C/C++ #################\n",
    "# The below function gives the ordinals related to each passwords and computes the hash values based on\n",
    "# coefficients that are stored on bloom filter configuration\n",
    "@jit(nopython=True, nogil=True, parallel=True, cache=True)\n",
    "def parallel(x,coef,hash_size):\n",
    "    return (x*coef).sum(axis=1)%hash_size\n",
    "# This function gives as input a passwords as string, computes the ordinals as list of integers and \n",
    "# call the parallel function to generate the hash values  \n",
    "def hash_gen(line, conf):\n",
    "    x = np.array(list(map(ord, line[0:-1])))\n",
    "    return parallel(x,conf.coef,conf.hash_size)\n",
    "\n",
    "# It initialize the bloom filter with all false values, streams the passwords1.txt, gets the hash values, updates\n",
    "# the bloom filter, checks the duplicated passwords in passwords2.txt \n",
    "def BloomFilter(passwords1 , passwords2 , bloom_filter_conf):\n",
    "    start_time = time.time()\n",
    "    bloom_filter = np.zeros(bloom_filter_conf.hash_size, dtype=bool)\n",
    "    num_inserted_pass = 0\n",
    "    # streaming passwords1.txt\n",
    "    with open(passwords1) as f:\n",
    "        for line in tqdm(f):\n",
    "            bloom_filter[hash_gen(line = line, conf = bloom_filter_conf)] = True        \n",
    "            num_inserted_pass += 1\n",
    "    duplicated_pass_num = 0\n",
    "    passes_num = 0\n",
    "    # streaming passwords2.txt\n",
    "    with open(passwords2) as f:\n",
    "        for line in tqdm(f):\n",
    "            passes_num += 1\n",
    "            # check duplication\n",
    "            if all(bloom_filter[hash_gen(line = line, conf = bloom_filter_conf)]):\n",
    "                duplicated_pass_num += 1\n",
    "    end_time = time.time()\n",
    "    print('Number of hash function used: ', bloom_filter_conf.hash_funcs_num)\n",
    "    print('Number of duplicates detected: ', duplicated_pass_num)\n",
    "    k = bloom_filter_conf.hash_funcs_num\n",
    "    m = bloom_filter_conf.hash_size\n",
    "    n = num_inserted_pass\n",
    "    e = math.exp\n",
    "    # The probability of false positive in theory \n",
    "    print('Probability of false positives: ', (1-e(-k*n/m))**k)\n",
    "    print('Execution time: ', end_time - start_time, ' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_size = 10000000019 #It is a huge prime number\n",
    "string_size = 20\n",
    "hash_funcs_num = 20\n",
    "bloom_filter_conf = hash_configuration('bloom filter', hash_size, string_size, hash_funcs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000000it [14:03, 118514.84it/s]\n",
      "39000000it [05:47, 112384.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hash function used:  20\n",
      "Number of duplicates detected:  14000000\n",
      "Probability of false positives:  1.467177285944451e-15\n",
      "Execution time:  1191.0610814094543  secs\n"
     ]
    }
   ],
   "source": [
    "BloomFilter('passwords1.txt', 'passwords2.txt', bloom_filter_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above this algorithm takes less than 20 minutes to insert the passwords in the bloom filter and check the duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### PySpark: parallel computing on single machine ####################\n",
    "# In order to compute exact number of false positive we start with computing \n",
    "# the true number of duplicated passwords inside the two .txt files\n",
    "# the difference between duplicated number extrated from bloom filter and true number of duplicated\n",
    "# passwords is the exact number of real false positive\n",
    "# in this regard the below varibales are defined:\n",
    "# pass1_num: number of unique passwords in passwords1.txt\n",
    "# pass2_num: number of unique passwords in passwords2.txt\n",
    "# union_num: number of unique passwords in passwords1.txt and passwords2.txt\n",
    "# the pass2_num - (union_num - pass1_num) will give the true number of duplicated passwords\n",
    "def duplicate(passwords1 , passwords2):\n",
    "    start_time = time.time()\n",
    "    sc = SparkContext('local[*]', 'Find dupicated passwords')\n",
    "    pass1_rdd = sc.textFile(passwords1)\n",
    "    pass1_num = pass1_rdd.distinct().count()\n",
    "    pass2_rdd = sc.textFile(passwords2)\n",
    "    pass2_num = pass2_rdd.distinct().count()\n",
    "    dup_rdd = pass2_rdd.union(pass1_rdd).distinct()\n",
    "    union_num = dup_rdd.count()\n",
    "    end_time = time.time()\n",
    "    print('Execution time: ', end_time - start_time, ' secs')\n",
    "    print('The number of exact duplicated passwords is: ', pass2_num - (union_num - pass1_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  715.9290888309479  secs\n",
      "The number of exact duplicated passwords is:  14000000\n"
     ]
    }
   ],
   "source": [
    "duplicate('passwords1.txt', 'passwords2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to this function using Map/Reduce (PySpark), we can evaluate the exact number of duplicated passwords inside the two .txt files in less than 12 minutes.\n",
    "As we can see this number is equal to the one we get from the bloom filter so it means that there isn't any false positive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
